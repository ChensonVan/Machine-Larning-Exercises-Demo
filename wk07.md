### 7  SVM - Support Vector Machine

#### 7. 1 Optimisation Objective

$$
h_\theta (x) = \frac 1 {1 + e^{-\theta^Tx}}	\\
z = -\theta^Tx
$$

![](http://p1.bqimg.com/567571/ccebe58b8c408be2.png)

![](http://p1.bpimg.com/567571/08cde1201ceffdf4.png)

**Why we need do that?**



#### 7.2 Hypothesis Function

##### 7.2.1 Logistic Regression

$$
\frac 1 m \sum_{i=1}^m [ y^{(i)} * (-log(h_\theta(x^{(i)})) + (1 - y^{(i)}) * (-log(1 - h_\theta(x^{(i)}))) ] + \frac \lambda {2m} \sum_{j=1}^n \theta_j^2
$$

##### 7.2.2 Support Vector Machine

$$
C \sum_{i=1}^m [y^{(i)} * cost_1(\theta^Tx^{(i)}) + (1 - y^{(i)}) * cost_0(\theta^Tx^{(i)})] + \frac 1 2 \sum_{j=1}^n \theta_j^2, 
\space \space \space \space \space C = \frac 1 \lambda
$$

**Analysis：**

为了使得cost function取得最小值，我们令C\*W + P部分中，C\*W为零。即：

1. 当 y = 1时， cost1 = 0，所以 **z >= 1**
2. 当 y = 0时， cost0 = 0，所以 **z <= -1**

![](http://i1.piimg.com/567571/f12b34463ba1fa44.png)



**Note：**1. cost0 and cost1 对应的是上图中左右两边的cost function，因为y=0和y=1的目标函数。

2. 常数C取一个很大的值时比较好。因为**C*W + P**， 所以C大则W会变小，即相对penality就会变大，W会变小
3. 为什么要重新选定一个cost function ？（**逻辑回归的临界点为0，但是SVM的临界点是1，所以SVM更加精确。** ）
4. 对应的线性逻辑回归？即次数不大于1的？
5. Decision Boundary 不是一条直线的情况



#### 7.3 Large Margin Classifier

```
结论：常数C取一个比较大的值比较容易获得Large Margin Classifier
C大，则比较容易获得
```

![](http://p1.bpimg.com/567571/ffb73c13bac7db8f.png)

**以上为两类分布比较均匀的时候，Decision Boundary为图中黑色的线，所有点离黑色的距离都相对比较大比较均匀，但是当存在干扰点的时候如下图，Decision Boundary会由黑色变为粉红色。所以C的取值不能太大，也不能太小。需要求出最优解**

![](http://p1.bqimg.com/567571/d8b0b0072aa7c3c9.png)

#### 7.4 Mathmatics Behind Large Margin Classification

##### 7.4.1 Vector Inner Product

![](http://i1.piimg.com/567571/8ebc858903b32e8c.png)

**Note：**

1. 如何求投影p的值？


2. 当角度 < 90°，p为正数。当角度 > 90°时，p为负数。

3. $$
   向量内积：u^Tv = ||u|| · ||v|| · cosθ = ||u|| · p_{v,u} = ||v|| · p_{u,v} = u_1v_1+u_2v_2
   $$






##### 7.4.2 SVM Cost Function

$$
C \sum_{i=1}^m [y^{(i)} * cost_1(\theta^Tx^{(i)}) + (1 - y^{(i)}) * cost_0(\theta^Tx^{(i)})] + \frac 1 2 \sum_{j=1}^n \theta_j^2, 
\space \space \space \space \space C = \frac 1 \lambda
$$

当C取一个一个很大的值时，cost function只剩下后面P的部分。 假设**θ0 = 0**
$$
\frac 1 2 \sum_{j=1}^n\theta^2_j = \frac 1 2 (\theta^2_0 + \theta^2_1 + ... + \theta^2_n) = \frac 1 2 (\theta^2_1 + ... + \theta^2_n)
= \frac 1 2 ||\theta||^2
$$
![](http://p1.bqimg.com/567571/4b804d7e42cfb375.png)

所以：
$$
\theta^Tx^{(i)} = p^{(i)}||\theta||	\\
p^{(i)}||\theta|| >= 1, if \space\space y^{(i)} = 1	\\
p^{(i)}||\theta|| <= -1, if \space\space y^{(i)} = 0	\\
p^{(i)}是点到向量\theta的projection，即点到Decision Boundary的距离
$$
上面我们讨论了，当C取到一个合适的、较大的数值时，SVM的cost function就只剩下后面P的部分，即
$$
\frac 1 2 ||\theta||^2
$$
我们要减小cost function，所以需要**减小θ的值**。

当θ取到一个比较小的值的时候，还需要满足上面讨论的：
$$
\theta^Tx^{(i)} = p^{(i)}||\theta||	\\
p^{(i)}||\theta|| >= 1, if \space\space y^{(i)} = 1	\\
p^{(i)}||\theta|| <= -1, if \space\space y^{(i)} = 0	\\
$$
**所以θ比较小时，只能增加p的值去满足p\*||θ|| >= 1 或者 p\*||θ||<= -1。**

**这样就保证了p的值比较大，即点到Decision Boundary的大间距。**



#### 7.5 Kernels

##### 7.5.1 Kernels & Similarity

首先，我们回想一下之前的logistic regression中对于non-linear 情况的拟合。

**Predict y = 1, if**
$$
\theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3x_2 + \theta_4x_1^2 + \theta_5x_2^2 + ... >= 0 \\
\theta_0 + \theta_1f_1 + \theta_2f_2 + \theta_3f_3 + \theta_4f_4 + \theta_5f_5 + ... >= 0
$$
即将fn定义为x的幂次项组合，如下：
$$
f_1 = x_1, f_2 = x_2, f_3 = x_1x_2, f_4 = x_1^2, f_5 = x_2^2, ...
$$
![](http://p1.bpimg.com/567571/1dc51f091e3a45c0.png)

但是在SVM中，我们要重新定义fn，引入Kernel的概念，即用 kernel function来表示fn。

![](http://p1.bqimg.com/567571/220a900a536966bb.png)

**Note:**

1. l 是landmark，且如果training sets里面的数量为n的话，则landmark的数量也为n。
2. 假设training sets数量为n，则对于一个新的example来说，可计算出n个新的特征f1...fn。然后用新的特征，对该example进行判断**（低维转为高维的过程）**
3. kernel function为guassian function。**当x与landmark l越接近时，两点的距离越小，值接近1**
4. ![](http://p1.bpimg.com/567571/715ea71242a4c57f.png)

##### 7.5.2 SVM with Kernels

![](http://p1.bpimg.com/567571/90910c2466c5cbfb.png)



对比之前的cost function，可以发现这里**θ和f(x)**跟之前的不同。

**在logistic regression 中，θ的维度为(n+1) x 1, 包含θ0， 且n为单个example的特征个数**

**在SVM with kernel中，f(x)的个数为m，其中m是training sets中的个数，所以θ的维度应该是(m+1)x1**



- **Steps**

  ![](http://p1.bqimg.com/567571/417a67419bd51ee7.png)
  - 1. 给定一组training sets，根据每个example，选取m个landmark点
    2. 计算每一个example与所有landmark的相识度，相同为1，非常不同接近为0。计算相识度的kernel function为Gaussian Function
    3. 最终，对于每一个example里面都可以计算出m个新的feature，所以对于这个training sets而言，会得到一个m*m的矩阵？
    4. 将得到的m*m的矩阵，代入到Hypothesis中，计算出θ的值。


##### 7.5.4 SVM parameters

![](http://p1.bqimg.com/567571/24abc84b6130bec4.png)

- **C**

$$
C = \frac 1 \lambda
$$

| Large C     | Small  λ     | Large θ     | Lower Bias      | High Variance    | Over Fitting      |
| ----------- | ------------ | ----------- | --------------- | ---------------- | ----------------- |
| **Small C** | **Large  λ** | **Small θ** | **Higher Bias** | **Low Variance** | **Under Fitting** |



- **σ**

  | Large σ     | more smoothly     | Higher Bias    | Lower Variance      | Under Fitting    |
  | ----------- | ----------------- | -------------- | ------------------- | ---------------- |
  | **Small σ** | **less smoothly** | **Lower Bias** | **Higher Variance** | **Over Fitting** |

  ![](http://i1.piimg.com/567571/3bc82f7fc83c84cd.png)

  ​

