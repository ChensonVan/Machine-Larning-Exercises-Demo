

### 5 Neural Networks (II)

#### 5.1 Cost Function and Backpropagation

![](http://i1.piimg.com/567571/d27b709b5a456ad6.png)

##### 5.1.1 Cost Function

- **Losistic Cost Function** (Only One Output)

![](http://i1.piimg.com/567571/29c45e8a2fb18a77.png)

- **Neural Networks Cost Function** (Multiple Outputs)

![](http://i1.piimg.com/567571/3ee9ccdf876cc8e1.png)

- **Gradient Computation**

$$
J(\Theta) = -\frac 1 m \sum_{i=1}^m \sum_{k=1}^K (y_k^i*log(h_\Theta(x^i))_k + (1-y_k^i)*log(1-(h_\Theta(x^i))_k))	\\
= -\frac 1 m \sum_{i=1}^m \sum_{k=1}^K (y_k^i*log((a_k^L)^i) + (1-y_k^i)*log(1-(a_k^L)^i))	\\

= -\frac 1 m \sum_{i=1}^m [log((a_1^L)^i), log(1-(a_1^L)^i] * \begin{bmatrix} y_1^i \\ 1-y_1^i\end{bmatrix} + ... + [log((a_K^L)^i), log(1-(a_K^L)^i] * \begin{bmatrix} y_K^i \\ 1-y_K^i\end{bmatrix}	\\

= -\frac 1 m \sum_{i=1}^m [[log((a_1^L)^i), log(1-(a_1^L)^i], ..., [log((a_K^L)^i), log(1-(a_K^L)^i]] * \begin{bmatrix} \begin{bmatrix} y_1^i \\ 1-y_1^i\end{bmatrix} \\ ... \\ \begin{bmatrix} y_K^i \\ 1-y_K^i\end{bmatrix}\end{bmatrix}	\\

= -\frac 1 m \begin{bmatrix} \begin{bmatrix} log((a_1^L)^1) \\ log(1 - (a_1^L)^1) \\ ... \\ log((a_K^L)^1) \\ log(1 - (a_K^L)^1) \end{bmatrix}^T, \begin{bmatrix} log((a_1^L)^2) \\ log(1 - (a_1^L)^2) \\ ... \\ log((a_K^L)^2) \\ log(1 - (a_K^L)^2) \end{bmatrix}^T, ..., \begin{bmatrix} log((a_1^L)^m) \\ log(1 - (a_1^L)^m) \\ ... \\ log((a_K^L)^m) \\ log(1 - (a_K^L)^m)\end{bmatrix}^T\end{bmatrix} * \begin{bmatrix} \begin{bmatrix} y_1^1 \\ 1-y_1^1 \\ ... \\ y_K^1 \\ 1-y_K^1\end{bmatrix} \\ ... \\ \begin{bmatrix} y_1^m \\ 1-y_1^m \\ ... \\ y_K^m \\ 1-y_K^m \end{bmatrix}\end{bmatrix}	\\

= -\frac 1 m \begin{bmatrix} \begin{bmatrix} log((a_1^L)^1) \\ ... \\ log((a_K^L)^1) \\ log(1 - (a_K^L)^1) \\...\\ log(1 - (a_1^K)^1) \end{bmatrix}^T, \begin{bmatrix} log((a_1^L)^2) \\...\\ log((a_K^L)^2) \\ log(1 - (a_1^L)^2) \\...\\ log(1- (a_K^L)^2) \end{bmatrix}^T, ..., \begin{bmatrix} log((a_1^L)^m) \\..\\ log((a_K^L)^m)\\ log(1-(a_1^L)^m) \\..\\ log(1 - (a_K^L)^m)\end{bmatrix}^T\end{bmatrix} * \begin{bmatrix} \begin{bmatrix} y_1^1 \\..\\ y_K^1 \\ 1 - y_1^1 \\...\\ 1 - y_K^1\end{bmatrix} \\ ... \\ \begin{bmatrix} y_1^m \\...\\ y_K^m \\1 - y_1^m \\...\\ 1-y_K^m \end{bmatrix}\end{bmatrix} 	\\

= -\frac 1 m [(log(a^1))^T, ..., (log(a^m))^T, (log(1 - a^1))^T, ..., (log(1 - a^m))^T] * \begin{bmatrix} y^1 \\ ... \\y^m\\1 -  y^1 \\ 1 - y^m \end{bmatrix} 	\\

= -\frac 1 m [log(a), log(1 - a)] * \begin{bmatrix} y \\ 1 - y \end{bmatrix}	\\

Let: a^i = \begin{bmatrix} a^i_1 \\ a^i_2 \\...\\ a^i_K\end{bmatrix}, a = [(a^1)^T, (a^2)^T, ..., (a^m)^T] = \begin{bmatrix} a^1\\ a^2 \\...\\a^m \end{bmatrix}
$$



#### 5.2 FP & BP

##### 5.2.1 FP

![](http://p1.bqimg.com/567571/3e39847289a46d03.png)
$$
L = 4	\\
S_1 = 2 (not \space counting \space bias \space unit)	\\
S_2 = 2, \space S_3 = 2	\\
S_4 = S_L =  1 = K ( output \space layer)
$$

$$
a^{(1)} = x ==> add \space a_0^{(1)} ==> z^{(2)} = \Theta^{(1)} * a^{(1)}	\\
a^{(2)} = g(z^{(2)}) ==> add \space a_0^{(2)} ==> z^{(3)} = \Theta^{(2)} * a^{(2)}	\\
a^{(3)} = g(z^{(3)}) ==> add \space a_0^{(3)} ==> z^{(4)} = \Theta^{(3)} * a^{(3)}	\\
h_\Theta(x) = a^{(4)} = g(z^{(4)})
$$

![](http://p1.bpimg.com/567571/978b25ea41e67d96.png)

##### 5.2.2 BP

- In FP, the algorithm will produce a output, wichi may be a single real number, but can also be a vector.

- BP take the output from FP, **compares it to the real value(y)** and calculates how wrong the networks.

  - ​

- It then, using the **error** we just have calculated, **back-calculates the error** associated with each unit from the preceding layer

  - $$
    \delta^L_j = a^L_j - y_j, then	\\
    \delta^{(3)} = (\Theta^{(3)})^T * \delta^{(4)} .* g'(z^{(3)})	\\
    \delta^{(2)} = (\Theta^{(2)})^T * \delta^{(3)} .* g'(z^{(2)})	\\
    \delta^{(L-1)}, \delta^{(L-2)}, ..., \delta^{(2)}	\\
    \Delta_{ij}^{(l)} = 0 ( for \space all \space l, i, j)
    $$


  ​

- Thes "errors" measurements for each unit can be used to calculate the **partial derivatives **(that's the funciton of BP).

  - According to these **partial derivatives**, we can calculate the **Gradient descent**.

  - $$
    \frac {\partial (J(\Theta))} {\partial (\Theta^{(l)}_{ij})}  = 
    \frac {\partial (J(\Theta))} {\partial (a^{(l+1)}_{i})} * 
    \frac {\partial (a^{(l+1)}_{i})} {\partial (z^{(l+1)}_{i})} * 
    \frac {\partial (z^{(l+1)}_{i})} {\partial (\Theta^{(l)}_{ij})}		\\
    J(\Theta) = f(a^{(l+1)}_i);		\\ 
    a^{(l+1)}_i = g(z^{l+1}_i); 	\\
    z^{(l+1)}_i = f(\Theta^{(t)}_{i,j})
    $$

- We use then partial derivatives with gradient descent to try minimize the cost function and update all the **theta values**.

- **Backpropagation Algorithm**

![](http://p1.bqimg.com/567571/c35c64626c3d57c3.png)

##### 5.2.3 Gradient Checking

- Representation

$$
\frac \partial {\partial \Theta}J(\Theta) \approx  \frac {J(\Theta + \epsilon) - J(\Theta - \epsilon)} {2\epsilon}, such \space as \space \varepsilon = 10^{-4}
$$

- Implementation

  ```octave
  eplison = 1e-4;
  for i = 1:n,
  	thetaPlus = theta;
  	thetaPlus(i) += epsilon;
  	thetaMinus = theta;
  	thetaMinus(i) -= epsilon;
  	gradApprox(i) = (J(thetaPlus) - J(thetaMinus)) / (2 * epsilon)
  end;
  ```



##### 5.2.4  Random Initialization

- Representation

![](http://p1.bqimg.com/567571/20bcaf2f54775e57.png)



- Implementation

  ```
  % If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.

  Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;
  Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;
  Theta3 = rand(1 ,11) * (2 * INIT_EPSILON) - INIT_EPSILON;
  ```