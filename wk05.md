### 5 Neural Networks (II)

#### 5.1 Cost Function and Backpropagation

![](http://i1.piimg.com/567571/d27b709b5a456ad6.png)

##### 5.1.1 Cost Function

- **Losistic Cost Function** (Only One Output)

![](http://i1.piimg.com/567571/29c45e8a2fb18a77.png)

- **Neural Networks Cost Function** (Multiple Outputs)

![](http://i1.piimg.com/567571/3ee9ccdf876cc8e1.png)

- **Deduction**

$$
J(\theta) = -\frac 1 m \sum_{i=1}^m \sum_{k=1}^K (y_k^i*log(h_\theta(x^i))_k + (1-y_k^i)*log(1-(h_\theta(x^i))_k))	\\
= -\frac 1 m \sum_{i=1}^m \sum_{k=1}^K (y_k^i*log((a_k^L)^i) + (1-y_k^i)*log(1-(a_k^L)^i)	\\

= -\frac 1 m \sum_{i=1}^m [log((a_1^L)^i), log(1-(a_1^L)^i] * \begin{bmatrix} y_1^i \\ 1-y_1^i\end{bmatrix} + ... + [log((a_K^L)^i), log(1-(a_K^L)^i] * \begin{bmatrix} y_K^i \\ 1-y_K^i\end{bmatrix}	\\

= -\frac 1 m \sum_{i=1}^m [[log((a_1^L)^i), log(1-(a_1^L)^i], ..., [log((a_K^L)^i), log(1-(a_K^L)^i]] * \begin{bmatrix} \begin{bmatrix} y_1^i \\ 1-y_1^i\end{bmatrix} \\ ... \\ \begin{bmatrix} y_K^i \\ 1-y_K^i\end{bmatrix}\end{bmatrix}	\\

= -\frac 1 m \begin{bmatrix} \begin{bmatrix} log((a_1^L)^1) \\ log(1 - (a_1^L)^1) \\ ... \\ log((a_K^L)^1) \\ log(1 - (a_K^L)^1) \end{bmatrix}^T, \begin{bmatrix} log((a_1^L)^2) \\ log(1 - (a_1^L)^2) \\ ... \\ log((a_K^L)^2) \\ log(1 - (a_K^L)^2) \end{bmatrix}^T, ..., \begin{bmatrix} log((a_1^L)^m) \\ log(1 - (a_1^L)^m) \\ ... \\ log((a_K^L)^m) \\ log(1 - (a_K^L)^m)\end{bmatrix}^T\end{bmatrix} * \begin{bmatrix} \begin{bmatrix} y_1^1 \\ 1-y_1^1 \\ ... \\ y_K^1 \\ 1-y_K^1\end{bmatrix} \\ ... \\ \begin{bmatrix} y_1^m \\ 1-y_1^m \\ ... \\ y_K^m \\ 1-y_K^m \end{bmatrix}\end{bmatrix}	\\

= -\frac 1 m \begin{bmatrix} \begin{bmatrix} log((a_1^L)^1) \\ ... \\ log((a_K^L)^1) \\ log(1 - (a_K^L)^1) \\...\\ log(1 - (a_1^K)^1) \end{bmatrix}^T, \begin{bmatrix} log((a_1^L)^2) \\...\\ log((a_K^L)^2) \\ log(1 - (a_1^L)^2) \\...\\ log(1- (a_K^L)^2) \end{bmatrix}^T, ..., \begin{bmatrix} log((a_1^L)^m) \\..\\ log((a_K^L)^m)\\ log(1-(a_1^L)^m) \\..\\ log(1 - (a_K^L)^m)\end{bmatrix}^T\end{bmatrix} * \begin{bmatrix} \begin{bmatrix} y_1^1 \\..\\ y_K^1 \\ 1 - y_1^1 \\...\\ 1 - y_K^1\end{bmatrix} \\ ... \\ \begin{bmatrix} y_1^m \\...\\ y_K^m \\1 - y_1^m \\...\\ 1-y_K^m \end{bmatrix}\end{bmatrix} 	\\

= -\frac 1 m [(log(a^1))^T, ..., (log(a^m))^T, (log(1 - a^1))^T, ..., (log(1 - a^m))^T] * \begin{bmatrix} y^1 \\ ... \\y^m\\1 -  y^1 \\ 1 - y^m \end{bmatrix} 	\\

= -\frac 1 m [log(a), log(1 - a)] * \begin{bmatrix} y \\ 1 - y \end{bmatrix}	\\

Let: a^i = \begin{bmatrix} a^i_1 \\ a^i_2 \\...\\ a^i_K\end{bmatrix}, a = [(a^1)^T, (a^2)^T, ..., (a^m)^T] = \begin{bmatrix} a^1\\ a^2 \\...\\a^m \end{bmatrix}
$$



#### 5.2 FP & BP

##### 5.2.1 FP

![](http://p1.bqimg.com/567571/3e39847289a46d03.png)



##### 5.2.2 BP

##### 5.2.3 Gradient Checking

- Representation

$$
\frac \partial {\partial \Theta}J(\Theta) \approx  \frac {J(\Theta + \epsilon) - J(\Theta - \epsilon)} {2\epsilon}, such \space as \space \varepsilon = 10^{-4}
$$

- Implementation

  ```octave
  eplison = 1e-4;
  for i = 1:n,
  	thetaPlus = theta;
  	thetaPlus(i) += epsilon;
  	thetaMinus = theta;
  	thetaMinus(i) -= epsilon;
  	gradApprox(i) = (J(thetaPlus) - J(thetaMinus)) / (2 * epsilon)
  end;
  ```



##### 5.2.4  Random Initialization

- Representation

![](http://p1.bqimg.com/567571/20bcaf2f54775e57.png)



- Implementation

  ```
  % If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.

  Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;
  Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;
  Theta3 = rand(1 ,11) * (2 * INIT_EPSILON) - INIT_EPSILON;
  ```

