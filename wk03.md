### Logistic Regression

### 3.1 Linear Regression

Linear regression map all predictions **greater than 0.5** as a **1** and all **less then 0.5** as a **0**. This method doesn't work well when the classification is not actually a linear function. For some extra situation, there are som extra points will affect the linea, see the below picture.

![](http://p1.bpimg.com/567571/8103826bbd2cb8c0.png)



### 3.2 Hypothesis Representation

$$
0 \leq h_\theta(x) \leq 1 	\\
h_\theta(x) = g(\theta^Tx) = P(y = 1 | x; \theta)	  \\
g(z) = \frac 1 {1 + e^{-z}}	\space\space\space\space\space\space\space(Sigmoid/Logistic\space function) \\
***P(y = 0 | x; \theta) + P(y = 1 | x; \theta) = 1
$$

- What does **P**(y=1|x; θ) mean?

  Probability that y = 1, given x, parameterized by 0/1.

  When the probabiligy of y being 1 is **greater than 0.5**, then we can predict **y = 1**.

![](http://i1.piimg.com/567571/2d3d9bb2a4ad77de.png)



### 3.3 Decision Boundary

> **the property of the hypothesis  under the parameters** 

$$
\theta^TX \geq 0, \theta \rightarrow the \space boundary
$$

![](http://p1.bpimg.com/567571/e4f80a87b2ae6deb.png)

- θ is a column vector with the above values,

  so, θ' = [-1, 0, 0, 1, 1]

- **Question**:

  1. if we only have a θ vector, how to draw the boundary?
     - we have got **θ‘x** to draw the boundary in the above, there are **FOUR FEATURES**, that are **x1, x2, x1^2, x2^2** and not only x1 and x2.
  2. How to draw the boundary in **Octave?**

  ​





### 3.4 Cost Function

##### 3.4.1 Recap - Linear Regression

$$
J(\theta) = \frac 1 {m} \sum^m_{i=1} \frac 1 2(h_\theta(x^i) - y^i)^2
$$

$$
Cost(h_\theta(x^i), y^i) = \frac 1 2(h_\theta(x^i) - y^i)^2 \\
where \space h_\theta(x^i) = predict \space value \space of \space x^i 
$$
However, this is would be a non-convex function of hte parameter's data. As there are a lot of **local optimism**, which make gradencent function cannot guaranteed to converge to the **global minimum**.

![](http://p1.bqimg.com/567571/3bf7365a866eb0a7.png)



##### 3.4.2 Logistic Regression

$$
h_\theta(x) = \frac 1 {1 + e^{-\theta^Tx}}	\\
J(\theta) = \frac 1 {m} \sum^m_{i=1} \frac 1 2(h_\theta(x^i) - y^i)^2	\\
where, cost(h_\theta(x^i), y) = \frac 1 2 h_\theta(x^{(i)} - y^i)^2  non-convex
$$

-  **A convex logistic regression cost function**

![](http://p1.bqimg.com/567571/956c191b87a829c4.png)



**0 <= hθ(x) <= 1**

Cost = 0 		if y = 1 and **hθ(x) = 1**

Cost -> inf	if y = 1 and **hθ(x) -> 0**

![](http://p1.bqimg.com/567571/c2aefe9cbc49f3ac.png)

**0 <= hθ(x) <= 1 **

Cost = 0 		if y = 0 and **hθ(x) = 0**

Cost -> inf	if y = 0 and **hθ(x) -> 1**

![](http://p1.bpimg.com/567571/29bedf934159354c.png)

- n features, n + 1 colums vactor for θ.
- The equation is the same as the linear regression but the gypohtesisi has changed.
- θ0 - θn should be updated simultaneously.
  - Could use a for-loop
  - Better would be avectorized implementation
- Feature scaling for gradient descent for logistic regression also applies here.
- cost(hθ(x) - y)  —> **cost for a training example**
- J(θ) = 1/m * sum(cost(hθ(x) - y))  —> **cost for all training set**

```
function [jval, gradent] = coustFunction(THETA, X, y)
```

- **Input** for the cost function is **θ**, which is a vector of the θ parameters

- **Two return values** from **cost function** are:

  - **jval**

    - **what is that actually mean and how compute jval?**
      1. That is the cost under **current THETA**
      2. calculate that according to the above formula.
    - if we want to see the change of J(θ), we have to store all jval to J_history ???

  - **gradient**

    - **What is the actually mean?**

      1. That should be the **new value of θ ???**
      2. gradient(1), gradient(2) and … gradient(n+1) could be calculated by vectorized implementation ???

    - (n + 1) x 1 vector

    - (n + 1) element are (n + 1) **partial derivative** terms

      each indexed value in **θ** gives the partial derivatives for the partial derivative of **J(θ) with θi**

    ![](http://p1.bpimg.com/567571/6ccd94fc475d085d.png)

    [导数，偏导数，方向导数，梯度](http://blog.csdn.net/walilk/article/details/50978864)

    梯度定义如下： 
    ​	**函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值**

    ​	函数f(x)在点(x0, x1, …, xn)关于所有点的偏导数的向量

    ​

    梯度的提出只为回答一个问题： 
    ​	**函数在变量空间的某一点处，沿着哪一个方向有最大的变化率****

    ​

    这里注意三点： 
    ​	**梯度是一个向量，即有方向，有大小**

    ​	**梯度的方向是最大方向导数的方向**

    ​	**梯度的值是最大方向导数的值**



**Question:** 

​	1. What is gradient descent actually doing?

​	2. compute **J(θ)** and the **derivatives for each θ** and **repeatly** that.

​	3. What is actually of all **partial derivatives for each θ**?



#### 3.4.3 Vectorized implementation (Multiple Classification)

**Question**: what is the dimension of theta in logistic regression? (K x (N + 1)) or **(N + 1) x 1**
$$
\theta \in R^{K * (N + 1)}
$$

##### 3.4.3.1 Cost Function (must be a convex function) 

$$
h = g(X*\theta) \space\space(sigmoid \space function)		\\
J(\theta) = \frac 1 m (-y^Tlog(h)- (1 - y)^Tlog(1-h))
$$

##### 3.4.3.2 Gradient Descent

$$
\theta := \theta - \frac \alpha m (X^T(g(X\theta) - \vec y))
$$

![](http://i1.piimg.com/567571/2a45455b17c073c9.png)



##### 3.4.3.3 Deduction

![](http://p1.bqimg.com/567571/ac365873a2c41db1.png)

Finally, we can find the result is **the same as Linear regression**.

![](http://i1.piimg.com/567571/8cd0f021e2d431d1.png)







##### 3.5 Multi-class Classification

![](http://p1.bqimg.com/567571/575f7bca46961bb3.png)

![](http://i1.piimg.com/567571/267553749b486e58.png)


$$
h_\theta^{(i)}(x) = P(y = i | x; \theta) \space \space \space(i = 1, 2, 3, ..., n) \\
\sum_{i=1}^nP(y = i | x; \theta) = 1
$$
And finally, we calculate the probability of  all features, then pick the class that **Maximize P(x=i)**.









##### 3.6 Overfitting

![](http://p1.bqimg.com/567571/6c8c256889b17944.png)

**(1) Reduce the number of features**

- Manually select which features to keep.
- Use a ***model selection algorithm*** (studied later in the course).

**(2) Regularization**

- Keep all the features, but reduce the **magnitude of parameters θj**.
- Regularization works well when we have a lot of slightly useful features.


- *But, how to choose the **lambda** in penalised cost function ????*

$$
J(\theta) = \frac 1 2m [\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\sum_{j=1}^n\theta_j^2]
$$

**Note**:	1. In regularisation cost function, **j** is from 1 rather than 0, so there are no penality for **θ0**.

​		2. Usually, we set a large number for **lambda** for genelizing a **small theta**.



##### 3.7 Regularized Linear Regression

##### 3.7.1 Gradient Descent

$$
Repeat {   \\ 
θ_0 := θ_0 − α \frac 1m ∑_{i=1}^m(h_θ(x^{(i)})−y^{(i)})x^{(i)}_0  \\
θ_j  := θ_j − α [(\frac 1m ∑_{i=1}^m(h_θ(x^{(i)})−y^{(i)})x^{(i)}_j) + \frac λ mθ_j]}, \space J \in \{1, 2, 3,  ..., n\} \\
θ_j := θ_j(1 − α \frac \lambda m)  - \alpha \frac 1 m ∑_{i=1}^m(h_θ(x^{(i)})−y^{(i)})x^{(i)}_j)
$$

The first tee in the above, 
$$
\theta_j(1 - \alpha \frac \lambda m) < 1
$$
intutively we can see **θ** reducing on every update and **smooth the boundary**.

**if λ is set to an extremely large value, the algorithm will results in undercutting.**

****![](http://i1.piimg.com/567571/b8c4905fea10b610.png)

![](http://i1.piimg.com/567571/dd6462806d4da995.png)



##### 3.7.2 Normal Equation

$$
\theta = (X^TX + \lambda L)^{-1} X^Ty \\
where \space L  = \begin{bmatrix} 0 & 0 & 0& ... & 0 \\ 0 & 1 & 0 & ... & 0 \\0 & 0 & 1 & ... & 0 \\ 0 & 0 & 0 & ... & 1 \end{bmatrix}
$$

L is (n + 1) * (n + 1) **diagonal maxtrix**, and which can make sure **the exist of invertible matrix of X**.



##### 3.7 Regularized Logistic Regression

the same as linear regression, but the hyphothesis function.

![](http://p1.bqimg.com/567571/15ed2f87a8f453e2.png)



![](http://p1.bqimg.com/567571/308d6ff686c6d8a2.png)













### Tutorial

[ML Lecture Note](http://www.holehouse.org/mlclass/06_Logistic_Regression.html)



