### Logistic Regression

##### 3.1 Linear Regression

Linear regression map all predictions **greater than 0.5** as a **1** and all **less then 0.5** as a **0**. This method doesn't work well when the classification is not actually a linear function. For some extra situation, there are som extra points will affect the linea, see the below picture.

![](http://p1.bpimg.com/567571/8103826bbd2cb8c0.png)



##### 3.2 Hypothesis Representation

$$
0 \leq h_\theta(x) \leq 1 	\\
h_\theta(x) = g(\theta^Tx) = P(y = 1 | x; \theta)	  \\
g(z) = \frac 1 {1 + e^{-z}}	\space\space\space\space\space\space\space(Sigmoid/Logistic\space function) \\
P(y = 0 | x; \theta) + P(y = 1 | x; \theta) = 1
$$

![](http://i1.piimg.com/567571/2d3d9bb2a4ad77de.png)



##### 3.3 Decision Boundary

> **the property of the hypothesis  under the parameters** 

$$
\theta^TX \geq 0, \theta \rightarrow the \space boundary
$$

![](http://p1.bpimg.com/567571/e4f80a87b2ae6deb.png)

##### 3.4 Cost Function

##### 3.4.1 Linear Regression

$$
J(\theta) = \frac 1 {m} \sum^m_{i=1} \frac 1 2(h_\theta(x^i) - y^i)^2
$$

$$
Cost(h_\theta(x^i), y^i) = \frac 1 2(h_\theta(x^i) - y^i)^2 \\
where \space h_\theta(x^i) = predict \space value \space of \space x^i 
$$
However, this is would be a non-convex function of hte parameter's data. As there are a lot of **local optimism**, which make gradencent function cannot guaranteed to converge to the **global minimum**.

![](http://p1.bqimg.com/567571/3bf7365a866eb0a7.png)



##### 3.4.2 Logistic Regression

![](http://p1.bqimg.com/567571/956c191b87a829c4.png)



**0 <= hθ(x) <=1**

Cost = 0 		if y = 1 and **hθ(x) = 1**

Cost -> inf	if y = 1 and **hθ(x) -> 0**

![](http://p1.bqimg.com/567571/c2aefe9cbc49f3ac.png)

**0 <= hθ(x) <=1**

Cost = 0 		if y = 0 and **hθ(x) = 0**

Cost -> inf	if y = 0 and **hθ(x) -> 1

![](http://p1.bpimg.com/567571/29bedf934159354c.png)



##### 3.4.3 Vectorized implementation

**Question**: what is theta in logistic regression? (n+1) * (n + 1) or (n + 1) * 1

##### 3.4.3.1 Cost Function (must be a convex function)

$$
h = g(X*\theta) \space\space(sigmod \space function)		\\
J(\theta) = \frac 1 m (-y^Tlog(h)- (1 - y)^Tlog(1-h))
$$

##### 3.4.3.2 Gradient Descent

$$
\theta := \theta - \frac \alpha m (X^T(g(X\theta) - \vec y))
$$

![](http://i1.piimg.com/567571/2a45455b17c073c9.png)



##### 3.4.3.3 Deduction

![](http://p1.bqimg.com/567571/ac365873a2c41db1.png)

Finally, we can find the result is **the same as Linear regression**.



##### 3.5 Multi-class Classification

![](http://p1.bqimg.com/567571/575f7bca46961bb3.png)
$$
h_\theta^{(i)}(x) = P(y = i | x; \theta) \space \space \space(i = 1, 2, 3, ..., n) \\
\sum_{i=1}^nP(y = i | x; \theta) = 1
$$
And finally, we calculate all probability of  features, then pick the class that **Maximize P(x=i)**.



##### 3.6 Overfitting

![](http://p1.bqimg.com/567571/6c8c256889b17944.png)

**(1) Reduce the number of features**

- Manually select which features to keep.
- Use a ***model selection algorithm*** (studied later in the course).

**(2) Regularization**

- Keep all the features, but reduce the **magnitude of parameters θj**.
- Regularization works well when we have a lot of slightly useful features.


- *But, how to choose the **lambda** in penalised cost function ????*

$$
J(\theta) = \frac 1 2m [\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\sum_{j=1}^n\theta_j^2]
$$

**Note**:	1. In regularisation cost function, **j** is from 1 rather than 0, so there are no penality for **θ0**.

​		2. Usually, we set a large number for **lambda** for genelizing a **small theta**.



##### 3.7 Regularized Linear Regression

##### 3.7.1 Gradient Descent

$$
Repeat {   \\ 
θ_0 := θ_0 − α \frac 1m ∑_{i=1}^m(h_θ(x^{(i)})−y^{(i)})x^{(i)}_0  \\
θ_j  := θ_j − α [(\frac 1m ∑_{i=1}^m(h_θ(x^{(i)})−y^{(i)})x^{(i)}_j) + \frac λ mθ_j]}, \space J \in \{1, 2, 3,  ..., n\} \\
θ_j := θ_j(1 − α \frac \lambda m)  - \alpha \frac 1 m ∑_{i=1}^m(h_θ(x^{(i)})−y^{(i)})x^{(i)}_j)
$$

The first tee in the above, 
$$
\theta_j(1 - \alpha \frac \lambda m) < 1
$$
intutively we can see **θ** reducing on every update and **smooth the boundary**.

**if λ is set to an extremely large value, the algorithm will results in undercutting.**

****![](http://i1.piimg.com/567571/b8c4905fea10b610.png)



##### 3.7.2 Normal Equation

$$
\theta = (X^TX + \lambda L)^{-1} X^Ty \\
where \space L  = \begin{bmatrix} 0 & 0 & 0& ... & 0 \\ 0 & 1 & 0 & ... & 0 \\0 & 0 & 1 & ... & 0 \\ 0 & 0 & 0 & ... & 1 \end{bmatrix}
$$

L is (n + 1) * (n + 1) **diagonal maxtrix**, and which can make sure **the exist of invertible matrix of X**.



##### 3.7 Regularized Logistic Regression

the same as linear regression, but the hyphothesis function.

![](http://p1.bqimg.com/567571/15ed2f87a8f453e2.png)



![](http://p1.bqimg.com/567571/308d6ff686c6d8a2.png)

















